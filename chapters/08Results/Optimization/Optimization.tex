\section{Optimization Results}
\label{sec:optRes}

The optimization results listed in this section are estimated using a $30$ particle two-dimensional quantum dot as reference system.

Profiling the code revealed, not surprisingly, that $~99\%$ of the runtime was spent diffusing the particles, i.e. spent in \verb+QMC::diffuse_walker+, regardless of whether Variational Monte-Carlo (VMC), Diffusion Monte-Carlo (DMC) or Adaptive Stochastic Gradient Descent (ASGD) was run. Optimizing the code then solely involved optimizing this function. 

The profiling tool of choice was \textit{KCacheGrind}, available at the Ubuntu Software Center. KCacheGrind lists relative time spent in functions graphically in blocks, whose size is proportional to the time spent inside the function, much like standard hard drive listing software does with files and file size.

Optimizations discussed in chapter \ref{ch:optAndGen} which are not mentioned in the following sections are optimizations implemented prior to the optimization process. These optimizations were considered ``standard optimizations'' and was thus implemented from start. 

\subsubsection{Storing the Slater matrix}

This optimization is described in detail in Section \ref{sec:storeSlater}. In addition to storing the Slater matrix, the calculation of $\tilde I$ from the inverse updating algorithm in Eq.~(\ref{eq:Itilde}) was done outside the main loops.

The percentages listed in the following table is the total time spent inside this specific function relative to all other functions. 

\begin{tabular}{ll}
 \verb+Orbitals::phi+ & \\
 \hline\hline
 Relative runtime used prior to optimization & 80.88\% \\
 Relative runtime used after optimization    & 8.2\% \\
 \hline
 Relative function speedup                   & 9.86
\end{tabular}

The speedup is not a result of optimizations within the function itself, but a result of far less calls to the function. If $\tilde I$ was calculated outside the main loops in the first place, the speedup would be far less significant. 


\subsubsection{Optimized Jastrow gradient}

This optimization described in this Section is discussed in detail in Section \ref{sec:optJastGrad}.

The percentages listed in the following table is the total time spent inside this specific function relative to all other functions. 

\begin{tabular}{ll}
 \verb+Jastrow::get_grad+ \& \verb+Jastrow::calc_dJ+ & \\
 \hline\hline
 Relative runtime used prior to optimization & 40\% \\
 Relative runtime used after optimization    & 5.24\% \\
 \hline
 Relative function speedup                   & 7.63
\end{tabular}

Exploiting the symmetries of the PadÃ© Jastrow gradient, in addition to calculating the new gradient based on the old, is in other words extremely efficient. Keep in mind that these results are for a high number of particles. For e.g. two particles, this optimization would not matter at all.

\subsubsection{Storing the orbital derivatives}

This optimization is covered in detail in Section \ref{sec:storeSlater}. Much like for the Slater matrix, the optimization in this case comes from the fact that the function itself is called fewer times, rather than being faster.

The percentages listed in the following table is the total time spent inside this specific function relative to all other functions. 


\begin{tabular}{ll}
 \verb+Orbitals.dell_phi+ & \\
 \hline\hline
 Relative runtime used prior to optimization & 56.27\% \\
 Relative runtime used after optimization    & 7.31\% \\
 \hline
 Relative function speedup                   & 7.70
\end{tabular}


\subsubsection{Storing quantum number independent terms}

This optimization is covered in detail in Section \ref{sec:optSPWFqnumIndie}. The result of the optimization is a reduction in the number of exponential function calls, which means a more efficient calculation of single particle states, their gradients and Laplacians.

The percentages listed in the following table is the total time spent inside this specific function relative to all other functions. 

\begin{tabular}{ll}
 \verb+Orbitals::phi+ \& \verb+Orbitals::dell_phi+ & \\
 \hline\hline
 Relative runtime used prior to optimization & 5.85\% \\
 Relative runtime used after optimization    & 0.13\% \\
 \hline
 Relative function speedup                   & ~45
\end{tabular}

This result is not surprisingly equal to $15\cdot 3$, since a $30$ particle quantum dot has $15$ unique quantum numbers. One set is used by the orbitals, and two by their gradients (the Laplacian is not a part of the diffusion). Prior to this optimization, $45$ exponential calls were needed to fill a row in the Slater matrix and the derivative matrix; this has been reduced to one.

\subsubsection{Overall optimization}

Combining all the optimizations listed in this chapter, the final runtime was reduced to $5\%$ of the original. The final scaling is presented in Figure \ref{fig:scaling}.

\begin{figure}[h]
 \begin{center}
  \subfigure{\includegraphics[scale=0.35]{../Graphics/scaling.png}}
  \subfigure{\includegraphics[scale=0.35]{../Graphics/scaling_loglog.png}} \\
  \caption{Scaling of the code with respect to the number of particles $N$ based on VMC calculations with $10^6$ cycles with $10^5$ thermalization steps run on eight processors. The figures are split into a low $N$ region and a high $N$ region. Only two dimensional quantum dots and atoms are displayed in the high $N$ figure. The figures to the right contain the same data as the figures to the left, however, displayed using logarithmic axes. As expected, the two-dimensional quantum dots (denoted Qdots 2D) are lowest on CPU-time and the homonuclear diatomic molecules are highest (denoted Molecules). The logarithmic figures clearly show a linear trend, implying a underlying power law.}
  \label{fig:scaling}
 \end{center}
\end{figure}

The following power laws are deduced based on linear regression of the above figures for $N > 2$

\begin{tabular}{l|c}
System & Scaling \\
\hline
Two dimensional quantum dots & $N^{2.1038}$ \\
Three dimensional quantum dots & $N^{2.1008}$ \\
Atoms & $N^{1.8119}$ \\
Homonuclear diatomic molecules & $N^{1.8437}$ \\ 
\end{tabular}


As the number of particles increase, the spatial dimensions contribution to the scaling becomes negligible compared to the number of particles, rendering two dimensional quantum dots and atoms similar in runtime. This is expected since there are far more matrices in the code of dimensions $N\times N$ than $N \times d$, where $d$ denotes the dimension. 

The Jastrow factor, inverse updating, etc., all involve the same computations for all systems, hence the reason why the atomic systems scale better than the quantum dots has to originate from the efficiency of the single particle wave functions. Consider for example the third single particle wave function for the hydrogen-like orbitals:

\begin{equation}
 \phi_3 = x.
\end{equation}

The corresponding expression for two dimensional quantum dots is

\begin{equation}
  \phi_3 = 2k^2y^2 - 1.
\end{equation}

It is obvious that the orbital for quantum dots contain a higher computational cost for the processor than the one for atoms. Comparing the expressions listed for quantum dots in Appendix \ref{appendix:SymPyHO3D} and Appendix \ref{appendix:SymPyHO} with those for atoms in Appendix \ref{appendix:SymPyHydro}, it is apparent that this trend is consistent.

The fact that the molecules scale almost identical to the atoms demonstrates the efficiency of the implementation. 





