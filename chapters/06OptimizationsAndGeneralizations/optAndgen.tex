\chapter{Generalization and Optimization}

There is a big difference in strategy between writing code for a specific problem, and creating a general solver. A general Quantum Monte-Carlo solver involves several layers of complexity, such as support for different potentials, single particle bases, sampling models, etc., which may easily lead to a combinatoric disaster if the planning is not done right. 

This chapter will cover the underlaying assumptions and goals in Section \ref{sec:AssGoal}, the ..

\section{Underlying Assumptions and Goals}
\label{sec:AssGoal}

Thousands of lines of code should be written once and for all\footnote{This, however, is rarely the case. The code used in this thesis has been completely restructured four times.}. This section will cover the assumptions made and the goals set, preliminary to the coding process.

\subsection{Assumptions}
\label{sec:ass}

The code scheme was laid down based on the following assumptions

\cfbox{3cm-3pt}{
\begin{enumerate}[label=\textbf{(\roman{*})}, ref=(\roman{*}), align=left]
 \item The constituents of the simulated systems are either pure fermionic or pure bosinic. \label{it::ass::pureFermBos}
 \item A fermionic system is a two-level system described by one Slater determinant pr. spin eigenvalue\footnote{The actual code is developed to easily add support for $n$-level systems.}. \label{it::ass::2level}
 \item The trial wave function of a fermionic system is a single determinant. \label{it::ass::fermiSingleDet}
 \item A bosonic system is modeled by all particles being in the same assumed single particle ground state. \label{it::ass::bosCondensate}
 \vspace{0.3cm}
\end{enumerate}
}

Other assumptions, such as the time-independence of the Hamiltonian, arise due to the QMC solver, not the specific implementation, and are hence covered in Chapter \ref{ch:QMC}.

\subsection{Generalization Goals}
\label{sec:genGoals}

The implementation should be general for:

\cfbox{3.17cm-1pt}{
\begin{enumerate}[label=\textbf{(\roman{*})}, ref=(\roman{*}), align=left]
 \item Fermions and Bosons. \label{it::gen::FermiAndBoson}
 \item Anisotropic- and isotropic diffusion, i.e. Brute Force - or Importance sampling. \label{it::gen::BF_IS}
 \item Different minimization algorithms. 
 \item Any Jastrow factor.
 \item Any error estimation algorithm.
 \item Any single particle basis, including expanded single particle bases. \label{it::gen::SP_basis}
 \item Any combination of any potentials. \label{it::gen::pot}
 \vspace{0.2cm}
\end{enumerate}
}
 
In addition, the following constraint is set on solvers:

\cfbox{3.42cm-1pt}{
\begin{enumerate}[label=\textbf{(\roman{*})}, ref=(\roman{*}), align=left]
\setcounter{enumi}{7}
 \item Full numerical support for all values involving derivatives.\label{it::gen::numSupp}
 \vspace{0.2cm}
\end{enumerate}
}


The challenge is, despite the vast array of combinations, to preserve simplicity and structure as layers of complexity are added. If-testing inside the solvers in order to achieve generalization are in other words out of the question (see the next section).

\subsection{Optimization Goals}
\label{sec:optGoals}

Designed for the CPU, runtime optimizations are favored over memory optimizations. The following list may appear short, but every step brings immense amounts of complexity to the implementation

\cfbox{3.6cm}{
\begin{enumerate}[label=\textbf{(\roman{*})}, ref=(\roman{*}), align=left]
 \item Identical values should never be re-calculated. \label{it::opt::reCalc}
 \item Generalization should not be achieved through if-tests in repeated function calls, but rather through polymorphism (see Section \ref{sec:typeCastPoly}). \label{it::opt::noIF}
 \item Linear scaling of runtime and number of CPUs for large simulations. \label{it::opt::parScale}
 \vspace{0.2cm}
\end{enumerate}
}

\clearpage
\section{Specifics Regarding Generalization}

Generalization is achieved through the use of deep object orientation, i.e. polymorphism, rather than if-testing. These concepts are described in Section \ref{sec:OO}. The assumptions listed in Section \ref{sec:ass} are applied if not otherwise is stated.

For details regarding the implementation of methods, see the code in \cite{libBorealisCode}.

\subsection{Generalization Goals \ref{it::gen::FermiAndBoson}-\ref{it::gen::pot}}

As discussed in Section \ref{sec:manyBodyWFs}, the mathematical difference between fermions and bosons (of importance to QMC) is how the many-body wave functions are constructed from the single-particle basis. In the case of fermions, the expression is given in terms of two Slater determinants, while for bosons, it is simply the product of all states.

\vspace{0.2cm}
\begin{lstlisting}
double Fermions::get_spatial_wf(const Walker* walker) {
    using namespace arma;
    
    //Spin up times Spin down (determinants)
    return det(walker->phi(span(0, n2 - 1), span())) * det(walker->phi(span(n2, n_p - 1), span()));
}
\end{lstlisting}

\begin{lstlisting}
double Bosons::get_spatial_wf(const Walker* walker) {
 
    double wf = 1;
 
    //Using the phi matrix as a vector in the case of bosons.
    //Assuming all particles to occupy the same single particle state (neglecting permutations).
    for (int i = 0; i < n_p; i++){
      wf *= walker->phi(i);
    }
    
    return wf;
}
\end{lstlisting}

Fermion/Boson overloaded pure virtual methods exist for all methods involving evaluation of the many body wave function, e.g. the spatial ratio and the Laplacian sum. When the \verb+QMC+ solver asks the \verb+System*+ object for a spatial ratio, depending on whether Fermions or Bosons are loaded run-time, the Fermion or Boson spatial ratio is evaluated. 

This way of splitting the system class also takes care of optimization goal \ref{it::opt::noIF} in Section \ref{sec:optGoals} regarding no use of repeating if-tests. 

Similar polymorphic splitting is introduced in the following classes:

\begin{listliketab}
\storestyleof{itemize}
 \begin{tabular}{l l}
 \textbullet \,\verb+Orbitals+       & Hydrogen orbitals, harmonic oscillator, etc. \\
 \textbullet \,\verb+BasisFunctions+ & Stand-alone single particle wave functions. Initialized by \verb+Orbitals+. \\
 \textbullet \,\verb+Sampling+       & Brute force - or importance sampling. \\
 \textbullet \,\verb+Diffusion+      & Isotropic or Fokker-Planck. Automatically selected by \verb+Sampling+. \\
 \textbullet \,\verb+ErrorEstimator+ & Simple or Blocking. \\
 \textbullet \,\verb+Jastrow+        & Pad√© Jastrow - or no Jastrow factor. \\
 \textbullet \,\verb+QMC+            & VMC or DMC. \\
 \textbullet \,\verb+Minimizer+      & ASGD. Support for adding additional minimizers. \\
 \end{tabular}
\end{listliketab}

Implementing a new Jastrow Factor done simply by creating a new subclass of \verb+Jastrow+: Simple yet versatile. For more details, see Section \ref{sec:OO}. The splitting done in \verb+QMC+ is done to avoid rewriting a lot of general QMC code.

A detailed description of the generalization of potentials (point \ref{it::gen::pot}) is given in Section \ref{sec:typeCastPoly}.

\subsection{Generalization Goal \ref{it::gen::SP_basis} and Expanded bases}

An expanded single particle basis is implemented as a subclass of the \verb+Orbitals+ superclass. It is designed as a wrapper to an \verb+Orbitals+ subclass containing basis elements $\phi_\alpha(r_j)$. In addition to these elements, the class has a set of expansion coefficients $C_{\gamma\alpha}$ in which the new basis elements are constructed

\begin{equation}
\label{eq:ExpBasisSP}
 \psi_\gamma^\mathrm{Exp.}(r_j) = \sum_{\alpha=0}^{B - 1} C_{\gamma\alpha}\phi_\alpha(r_j) 
\end{equation}

where $B$ is the size of the expanded basis.

\vspace{0.5cm}
\begin{lstlisting}[language=C++]
class ExpandedBasis : public Orbitals {

...

protected:

    int basis_size;
    arma::mat coeffs;
    Orbitals* basis;
    
    //Hartree-Fock
    void calculate_coefficients();

};

\end{lstlisting}

In order to calculate the expansion coefficients, a \textit{Hartree-Fock} solver has been implemented. For a given set of orbitals, e.g. harmonic oscillator, everything that is needed in order to obtain an expanded basis, is to implement expressions for the one-body - and two-body interaction elements.

The implementation of Eq.~(\ref{eq:ExpBasisSP}) are done by overloading the original \verb+Orbitals.phi+ method

\vspace{0.5cm}
\begin{lstlisting}
double ExpandedBasis::phi(const Walker* walker, int particle, int q_num) {

    double value = 0;
    
    //Dividing basis_size by half assuming a two-level system.
    //In case of Bosons, expanding s.p. w.f. does not make sence.
    for (int m = 0; m < basis_size/2; m++) {
        value += coeffs(q_num, m) * basis->phi(walker, particle, m);
    }

    return value;

}
\end{lstlisting}

See Section \textbf{REF HF} for details regarding Hartree-Fock theory.

\subsection{Generalization Goal \ref{it::gen::numSupp}}

Functionality for evaluating derivatives numerically is important for two reasons; the first being debugging, the second being the cases where no closed-form expressions for the derivatives can be obtained\footnote{In most cases they can, however, in some cases the expressions are so heavy that they are not practical to use.}.

As an example, \verb+Orbitals.dell_phi+, which returns a single particle derivative, is virtual, and can be overloaded to call the numerical derivative implementation \verb+Orbitals.num_diff+\footnote{An alternative would be to perform the derivative on the full many-body wave function, however, this would demand another layer of polymorphism and make the code less intuitive.}. The same goes for the Jastrow factor and the variational derivatives in the minimizers. Similar implementations exist for the Laplacian.

The numerical implementations are finite difference schemes with error proportional to the squared step length.

\section{Optimizations due to a Single two-level Determinant}

Assumption \ref{it::ass::fermiSingleDet} unlocks the possibility to optimize the expressions involving the Slater-determinant dramatically. Similar optimizations for bosons due to assumption \ref{it::ass::bosCondensate} are considered trivial and will not be covered in detail. See the code in \cite{libBorealisCode} for details regarding bosons.

The full trial wave function $\Psi_T$ is given by Eq.~(\ref{eq:singleDeterminantTWF}). I will drop the function arguments to clean up the expressions. Written in terms of a spatial function $|D|$ and a Jastrow function $J$, we get

\newcommand{\PT}{|D^\uparrow||D^\downarrow|J}
\newcommand{\Du}{|D^\uparrow|}
\newcommand{\Dd}{|D^\downarrow|}
\newcommand{\Da}{|D^\alpha|}

\begin{equation}
 \Psi_T = \PT
\end{equation}

The Quantum Force becomes

\begin{eqnarray}
 F_i &=& 2\frac{\nabla_i\left(\PT\right)}{\PT} \nonumber\\
     &=& 2\left(\frac{\nabla_i\Du}{\Du} + \frac{\nabla_i\Dd}{\Dd} + \frac{\nabla_i J}{J}\right) \nonumber \\
     &=& 2\left(\frac{\nabla_i\Da}{\Da} + \frac{\nabla_i J}{J}\right)
\end{eqnarray}

where $\alpha$ is the spin configuration of particle $i$. The counterpart to $\alpha$ has no dependence of particle $i$ and will hence be zero in the expression above.

Equally for the Laplacians used in the local energy, we get

\begin{eqnarray}
 E_L &=& \sum_i \frac{1}{\Psi_T}\nabla^2_i \Psi_T + V \label{eq:localEsum}\\
  \frac{1}{\Psi_T}\nabla^2_i\Psi_T &=&  \frac{1}{\PT}\nabla^2_i \PT \nonumber\\
  &=& \frac{\nabla^2_i \Du}{\Du} + \frac{\nabla^2_i \Dd}{\Dd} + \frac{\nabla^2_i J}{J} \nonumber\\
  && +\,\, 2\frac{\left(\nabla_i\Du\right)\left(\nabla_i\Dd\right)}{\Du\Dd} + 2 \frac{\left(\nabla_i\Du\right)\left(\nabla_i J\right)}{\Du J} + 2 \frac{\left(\nabla_i\Dd\right)\left(\nabla_i J\right)}{\Dd J} \nonumber\\
  &=& \frac{\nabla^2_i \Da}{\Da} + \frac{\nabla^2_i J}{J} + 2\frac{\nabla_i\Da}{\Da} \frac{\nabla_i J}{J}
\end{eqnarray}

Finally, the expression for the $R_\psi$ ratio for Metropolis is

\begin{eqnarray}
 R_\psi &=& \frac{\Psi_T^\mathrm{new}}{\Psi_T^\mathrm{old}} \nonumber\\
        &=& \frac{\Du^\mathrm{new} \Dd^\mathrm{new} J^\mathrm{new}}{\Du^\mathrm{old} \Dd^\mathrm{old} J^\mathrm{old}} \nonumber\\
        &=& \frac{\Da^\mathrm{new}}{\Da^\mathrm{old}}\frac{J^\mathrm{new}}{J^\mathrm{old}} \label{eq:RpsiOpt}
\end{eqnarray}

where either the spin up or the spin down determinant is unchanged by the step. 

From these expressions it is clear that we get a halving of the dimensionality of the system by splitting the Slater determinants. Calculation the determinant of a $N\times N$ matrix is $\mathcal{O}(N^2)$ floating point operations (flops). This implies a speedup of four in estimating the determinants alone. 

\section{Optimizations due to Single-particle Moves}

Moving only one particle at the time (see the diffusion algorithm in Fig. \ref{fig:diffFlowChart}), means changing a single row in the Slater-determinant at the time. Changes to a single row means that almost every \textit{co-factor} remains unchanged. Since all of the expressions deduced in the previous section contains ratios of the spatial wave functions, expressing these determinants in terms of their co-factors should reveal cancellation of terms. 

\subsection{Optimizing the Slater ratios}
\label{sec:optSlaterRat}

The inverse of the Slater-matrix is given in terms of its \textit{adjugate} by the following relation \cite{linAlg} (spin-configuration parameter $\alpha$ will be skipped for now)

\begin{equation*}
 D^{-1} = \frac{1}{|D|}\mathrm{adj} D
\end{equation*}

The adjugate of a matrix is the transpose of the cofactor matrix $C$. Given in terms of matrix element equations, we have

\begin{eqnarray}
 D^{-1}_{ij} &=& \frac{C_{ji}}{|D|}\label{eq:invExpCofac} \\
 D_{ji} &=& \phi_i(r_j) \label{eq:slaterMatPhi}
\end{eqnarray}

Moreover, the determinant can be expressed as a \textit{cofactor expansion} around row $j$ (Kramer's rule)

\begin{equation}
\label{eq:cofacExp}
 |D| = \sum_i D_{ji} C_{ji}.
\end{equation}

The spatial part of the $R_\psi$ ratio is then (inserting Eq.~(\ref{eq:cofacExp}) into Eq.~(\ref{eq:RpsiOpt}))

\begin{equation}
\label{eq:RpsiCofac}
 R_S = \frac{\sum_i D_{ji}^\mathrm{new}C_{ji}^\mathrm{new}}{\sum_i D_{ji}^\mathrm{old}C_{ji}^\mathrm{old}}
\end{equation}

Let $j$ represent the moved particle, the $j$'th column of the cofactor matrix is unchanged when the particle moves (column $j$ depends on every column but its own). In other words,

\begin{equation}
 C_{ji}^\mathrm{new} = C_{ji}^\mathrm{old} = (D^\mathrm{old}_{ij})^{-1}|D^\mathrm{old}|,
\end{equation}

where the inverse relation of Eq.~(\ref{eq:invExpCofac}) has been used. Inserting this into Eq.~(\ref{eq:RpsiCofac}) yields

\begin{eqnarray}
  R_S &=& \frac{|D^\mathrm{old}|}{|D^\mathrm{old}|}\frac{\sum_i D_{ji}^\mathrm{new}(D_{ij}^\mathrm{old})^{-1}}{\sum_i D_{ji}^\mathrm{old}(D_{ij}^\mathrm{old})^{-1}} \nonumber\\
         &=& \frac{\sum_i D_{ji}^\mathrm{new}(D_{ij}^\mathrm{old})^{-1}}{I_{jj}} \nonumber
\end{eqnarray}

The diagonal element of the identity matrix is by definition unity. Inserting this fact combined with the relation from Eq.~(\ref{eq:slaterMatPhi}) yields the optimized expression for the ratio

\cfbox{4cm-4pt}{
\begin{equation}
\label{eq:spatialRatioDet}
 R_S = \sum_i \phi_i(r_j^\mathrm{new})(D_{ij}^\mathrm{old})^{-1}
\end{equation}
}

where $j$ is the currently moved particle. The sum $i$ spans the Slater matrix whose spin value matches that of particle $j$.


Similar reductions can be applied to all the Slater ratio expressions from the previous section, see refs. \cite{abInitioMC, morten}:

\cfbox{4cm}{
\begin{eqnarray}
 \frac{\nabla_i|D|}{|D|} &=& \sum_{k} \nabla_i\phi_k(r_i^\mathrm{new})(D_{ki}^\mathrm{new})^{-1} \\
  \frac{\nabla_i^2|D|}{|D|} &=& \sum_{k} \nabla_i^2\phi_k(r_i^\mathrm{new})(D_{ki}^\mathrm{new})^{-1}
\end{eqnarray}
}

where again, $k$ spans the Slater matrix whose spin values match that of the moved particle. Unlike for the ratio, $N/2$ of the gradients needs to be recalculated once a particle is moved (with $N$ being the number of particles). This is due to the fact that it is the new Slater inverse that is used, and not the old.

Closed form expressions for the derivatives and Laplacians of the single particle may be implemented and accessed when calling these functions to avoid expensive numerical calculations. See Appendices \textbf{REF SYMPYGEN} for a tabulation of closed form expressions.

\subsection{Optimizing the Inverse}

One might question the efficiency of calculating inverse matrices compared to brute force estimation of the determinants. However, as for the ratio in Eq.~(\ref{eq:spatialRatioDet}), using co-factor expansions we can construct an updating algorithm which dramatically decreases the cost of calculating the inverse of the new Slater matrix.

Let $i$ be the currently moved particle, the new inverse is given in terms of the old by the following expression \cite{abInitioMC, morten}

\cfbox{4.5cm-5pt}{
\begin{eqnarray}
 \tilde I_{ij} &=& \sum_l D_{il}^\mathrm{new}(D_{lj}^\mathrm{old})^{-1} \\
 \ \nonumber\\
 (D_{kj}^\mathrm{new})^{-1} &=& (D_{kj}^\mathrm{old})^{-1} - \frac{1}{R_S}(D_{ji}^\mathrm{old})^{-1}\tilde I_{ij} \qquad\qquad j \ne i\\
 (D_{ki}^\mathrm{new})^{-1} &=& \frac{1}{R_S}(D_{ki}^\mathrm{old})^{-1} \qquad\qquad\qquad\qquad\qquad\,\, \mathrm{else} \\
 \ \nonumber
\end{eqnarray}
}

This reduces the cost of calculating the inverse by an order of magnitude down to $\mathcal{O}(N^2)$.

Further optimization can be achieved by calculating the $\tilde I$ vector for particle $i$ prior to performing the loop over $k$ and $j$. Again, this loop should only update the inverse Slater matrix whose spin value correspond to that of the moved particle.

\subsection{Optimizing the Pad√© Jastrow factor Ratio}

As for the Green's function ratio is Eq.~(\ref{eq:MetropolisHastings}), the ratio between two Jastrow factors are best calculating as exponentiation the logarithm

\begin{eqnarray}
 \log \frac{J^\mathrm{new}}{J^\mathrm{old}} &=& \sum_{k<j = 1}^N \frac{a_{kj}r^\mathrm{new}_{kj}}{1 + \beta r^\mathrm{new}_{kj}} - \frac{a_{kj}r^\mathrm{old}_{kj}}{1 + \beta r^\mathrm{old}_{kj}} \\
                      &\equiv& \sum_{k<j = 1}^N g^\mathrm{new}_{kj} - g^\mathrm{old}_{kj} \label{eq:jastrowRatSTD}
\end{eqnarray}

The relative distances $r_{kj}$ behave much like the cofactors in Section \ref{sec:optSlaterRat}; changing $r_i$ only changes $r_{ij}$, in other words

\begin{equation}
 r^\mathrm{new}_{kj} = r^\mathrm{old}_{kj} \qquad k \ne i 
\end{equation}

which inserted into Eq.~(\ref{eq:jastrowRatSTD}) yields

\begin{eqnarray}
  \log\frac{J^\mathrm{new}}{J^\mathrm{old}} &=& \sum_{k<j \ne i} g^\mathrm{old}_{kj} - g^\mathrm{old}_{kj} + \sum_{j = 1}^N g^\mathrm{new}_{ij} - g^\mathrm{old}_{ij} \nonumber\\
                                            &=& \sum_{j = 1}^N a_{ij}\left(\frac{r^\mathrm{new}_{ij}}{1 + \beta r^\mathrm{new}_{ij}} - \frac{r^\mathrm{old}_{ij}}{1 + \beta r^\mathrm{old}_{ij}}\right)
\end{eqnarray}

Exponentiating both sides reveals the final optimized ratio 

\cfbox{4.6cm-2pt}{
\begin{equation}
 \frac{J^\mathrm{new}}{J^\mathrm{old}} = \exp \left[\sum_{j = 1}^N a_{ij}\left(\frac{r^\mathrm{new}_{ij}}{1 + \beta r^\mathrm{new}_{ij}} - \frac{r^\mathrm{old}_{ij}}{1 + \beta r^\mathrm{old}_{ij}}\right)\right]
\end{equation}
}


\section{Optimizing the Pad√© Jastrow Derivative Ratios}

The shape of the Pad√© Jastrow factor is general in the sense that its shape is independent of the system at hand. Calculating closed form expressions for the derivatives is then a process which can be done once and for all. 

\subsection{The Gradient}

Using the notation of Eq.~(\ref{eq:jastrowRatSTD}), the $x$ component of the Pad√© Jastrow gradient ratio for particle $i$ then becomes

\begin{equation}
 \frac{1}{J}\frac{\partial J}{\partial x_i} = \frac{1}{\prod_{k < l}\exp g_{kl}}\frac{\partial }{\partial x_i}\prod_{k < l}\exp g_{kl} 
\end{equation}

Using the product rule, only terms with $k$ or $l$ equal to $i$ survive the differentiation. In addition, the terms independent of $i$ will cancel the corresponding terms in the denominator. In other words, 

\begin{eqnarray}
  \frac{1}{J}\frac{\partial J}{\partial x_i} &=& \sum_{k \ne i} \frac{1}{\exp g_{ik}}\frac{\partial}{\partial x_i} \exp g_{ik} \nonumber\\
  &=& \sum_{k \ne i} \frac{1}{\exp g_{ik}}\exp g_{ik}\frac{\partial g_{ik}}{\partial x_i} \nonumber \\
  &=& \sum_{k \ne i} \frac{\partial g_{ik}}{\partial x_i} \nonumber \\
  &=& \sum_{k \ne i} \frac{\partial g_{ik}}{\partial r_{ik}}\frac{\partial r_{ik}}{\partial x_i}
\end{eqnarray}

\begin{eqnarray}
 \frac{\partial g_{ik}}{\partial r_{ik}} &=& \frac{\partial }{\partial r_{ik}} \left(\frac{a_{ik}r_{ik}}{1 + \beta r_{ik}}\right) \nonumber\\
  &=& \frac{a_{ik}}{1 + \beta r_{ik}} - \frac{a_{ik}r_{ik}}{(1 + \beta r_{ik})^2}\beta \nonumber \\
  &=& \frac{a_{ik}(1 + \beta r_{ik}) - a_{ik}\beta r_{ik}}{(1 + \beta r_{ik})^2}  \nonumber \\
  &=& \frac{a_{ik}}{(1 + \beta r_{ik})^2} \label{eq:jastrowDgikDrik}
\end{eqnarray}

\begin{eqnarray}
 \frac{\partial r_{ik}}{\partial x_i} &=& \frac{\partial }{\partial x_i} \sqrt{(x_i - x_k)^2 + (y_i - y_k)^2 + (z_i - z_k)^2} \nonumber \\
  &=& \frac{1}{2} 2(x_i - x_k) / \sqrt{(x_i - x_k)^2 + (y_i - y_k)^2 + (z_i - z_k)^2} \nonumber \\
  &=& \frac{x_i - x_k}{r_{ik}}
\end{eqnarray}

Combining these expressions yields

\begin{equation}
\label{eq:jastrowDerivX}
 \frac{1}{J}\frac{\partial J}{\partial x_i} = \sum_{k \ne i} \frac{a_{ik}}{r_{ik}}\frac{x_i - x_k}{(1 + \beta r_{ik})^2}
\end{equation}

When changing Cartesian variable in the differentiation, the only change to the expression is that the corresponding Cartesian variable changes in the numerator of Eq.~(\ref{eq:jastrowDerivX}). In other words, generalizing to the full gradient is done by substituting the Cartesian difference with the position vector difference.

\cfbox{4.8cm}{
\begin{equation}
 \frac{\nabla_i J}{J} = \sum_{k \ne i = 1}^N \frac{a_{ik}}{r_{ik}}\frac{\vec r_i - \vec r_k}{(1 + \beta r_{ik})^2}
\end{equation}
}


\subsection{The Laplacian}

The method of deducing the closed form expression for the Laplacian of the Pad√© Jastrow factor is identical to that of the gradient. The full calculation is done in ref. \cite{morten}. The expression becomes

\begin{equation}
\label{eq:jastrowLaplRaw}
 \frac{\nabla^2_i J}{J} = \left| \frac{\nabla_i J}{J}\right|^2 + \sum_{k \ne i = 1}^N \left(\frac{d-1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}} + \frac{\partial^2 g_{ik}}{\partial r_{ik}^2}\right) 
\end{equation}

where $d$ is the number of dimensions, arising due to the fact that the Laplacian, unlike the gradient, is a summation of contributions from all dimensions. A simple differentiation of Eq.~(\ref{eq:jastrowDgikDrik}) with respect to $r_{ik}$ yields

\begin{equation}
\label{eq:jastrowD2gikDrik2}
 \frac{\partial^2 g_{ik}}{\partial r_{ik}^2} = -\frac{2a_{ik}\beta}{(1 + \beta r_{ik})^3}
\end{equation}

Inserting Eq.~(\ref{eq:jastrowDgikDrik}) and Eq.~(\ref{eq:jastrowD2gikDrik2}) into Eq.~(\ref{eq:jastrowLaplRaw}) yields

\begin{eqnarray}
 \frac{\nabla^2_i J}{J} &=& \left| \frac{\nabla_i J}{J}\right|^2 + \sum_{k \ne i = 1}^N \left(\frac{d-1}{r_{ik}}\frac{a_{ik}}{(1 + \beta r_{ik})^2} - \frac{2a_{ik}\beta}{(1 + \beta r_{ik})^3}\right) \nonumber\\
  &=& \left| \frac{\nabla_i J}{J}\right|^2 + \sum_{k \ne i = 1}^N a_{ik}\frac{(d-1)(1 + \beta r_{ik}) - 2\beta r_{ik}}{r_{ik}(1 + \beta r_{ik})^3} \nonumber
\end{eqnarray}

which with a little cleanup results in

\begin{equation}
 \frac{\nabla^2_i J}{J} = \left| \frac{\nabla_i J}{J}\right|^2 + \sum_{k \ne i} a_{ik}\frac{(d-1) + (d-3)\beta r_{ik}}{r_{ik}(1 + \beta r_{ik})^3}
\end{equation}

The local energy calculation needs the sum of the Laplacians for all particles (see Eq.~(\ref{eq:localEsum})). In other words, the quantity of interest becomes

\begin{equation}
 \sum_i \frac{\nabla^2_i J}{J} = \sum_i \left|\frac{\nabla_i J}{J}\right|^2 + \sum_{i \ne k}^N a_{ik}\frac{(d-1) + (d-3)\beta r_{ik}}{r_{ik}(1 + \beta r_{ik})^3}
\end{equation}

However, due to the symmetry of $r_{ik}$, the second term count equal values twice. We can therefore optimize the calculation by only summing for $k>i$, and rather multiply the sum by two. Bringing it all together, we get

\cfbox{5cm}{
\begin{equation}
 \sum_i \frac{\nabla^2_i J}{J} = \sum_i \left|\frac{\nabla_i J}{J}\right|^2 + 2\sum_{i < k}^N a_{ik}\frac{(d-1) + (d-3)\beta r_{ik}}{r_{ik}(1 + \beta r_{ik})^3}
\end{equation}
}


\section{Further Optimizations}

The optimizations covered in this section will exclusively arise from point \ref{it::opt::reCalc} in section \ref{sec:optGoals}. Avoiding recalculation of expressions which could have been tabulated is a major source of speedup. 

rrel, r2, phimatrix, delphi, oppdatere jastrow gradient, qnum indie terms ++ a s a s d a d sd f s f s a sa dsa sda a a  ads ad  gf asrfs asdf as fas  a asd asd as 

\subsection{Tabulating Recalculated Data}

blah
sad
sda


sda
sdasda



\begin{equation}
r_{rel} = r_{rel}^T = \left( \begin{array}{ccccc}
0 & r_{12} & r_{13} & \cdots & r_{1N} \\
 & 0 & r_{23} & \cdots & r_{2N}  \\
 &  & \ddots & \ddots & \vdots \\
 & \cdots &  & 0 & r_{(N-1)N} \\
 &  &  &  & 0\end{array} \right).
\end{equation}

\begin{lstlisting}
 void Sampling::update_pos(const Walker* walker_pre, Walker* walker_post, int particle) const {

    ...

    //Updating the part of the r_rel matrix which is changed by moving the [particle]
    for (int j = 0; j < n_p; j++) {
        if (j != particle) {
            walker_post->r_rel(particle, j) = walker_post->r_rel(j, particle)
                    = walker_post->calc_r_rel(particle, j);
        }
    }
    
    ...

}
\end{lstlisting}


Functions such as Coulomb and the Jastrow Factor can then simply access these matrix elements.


Avoid if-testing spin:

\begin{equation}
 D^{-1} \equiv \left[(D^\uparrow)^{-1}\,(D^\downarrow)^{-1}\right]
\end{equation}

\begin{equation}
 D \equiv \left[D^\uparrow\,D^\downarrow\right] = \left[ \begin{array}{cccc}
\phi_1(r_0)     & \phi_1(r_1)     & \cdots & \phi_1(r_N)       \\
\phi_2(r_0)     & \phi_2(r_1)     & \cdots & \phi_2(r_N)       \\
\vdots          & \vdots          &        & \vdots            \\
\phi_{N/2}(r_0) & \phi_{N/2}(r_1) & \cdots & \phi_{N/2}(r_N)   \end{array} \right]
\end{equation}

\begin{equation}
 \nabla D \equiv \left[ \begin{array}{cccc}
\nabla \phi_1(r_0)     & \nabla \phi_1(r_1)     & \cdots & \nabla \phi_1(r_N)       \\
\nabla \phi_2(r_0)     & \nabla \phi_2(r_1)     & \cdots & \nabla \phi_2(r_N)       \\
\vdots                 & \vdots                 &        & \vdots                   \\
\nabla \phi_{N/2}(r_0) & \nabla \phi_{N/2}(r_1) & \cdots & \nabla \phi_{N/2}(r_N)   \end{array} \right]
\end{equation}

\begin{equation}
 \mathrm{d}\vec J_{ik} = -\mathrm{d}\vec J_{ki} \equiv \frac{a_{ik}}{r_{ik}}\frac{\vec r_i - \vec r_k}{(1 + \beta r_{ik})^2}
\end{equation}


\begin{equation}
 \mathrm{d}J = -\mathrm{d}J^T \equiv \left( \begin{array}{ccccc}
0 & \mathrm{d}J_{12} & \mathrm{d}J_{13} & \cdots & \mathrm{d}J_{1N} \\
 & 0 & \mathrm{d}J_{23} & \cdots & \mathrm{d}J_{2N}  \\
 &  & \ddots & \ddots & \vdots \\
 & (-) &  & 0 & \mathrm{d}J_{(N-1)N} \\
 &  &  &  & 0\end{array} \right).
\end{equation}


\begin{lstlisting}
void AlphaHarmonicOscillator::set_qnum_indie_terms(const Walker * walker, int i) {
   
   //k2 = alpha*omega 
   *exp_factor = exp(-0.5 * (*k2) * walker->get_r_i2(i));
}
\end{lstlisting}

for N particles, saves $N(1 + \frac{d}{2})$ exponential calls pr. particle pr. walker pr. cycle.

3000 cycles, ~1000 walkers, 100 blocks, 42 particles = $12.6\cdot 10^9$ exponential calls saved.

