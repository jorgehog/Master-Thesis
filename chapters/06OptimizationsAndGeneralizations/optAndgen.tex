\chapter{Generalization and Optimization}

There is a big difference in strategy between writing code for a specific problem, and creating a general solver. A general Quantum Monte-Carlo solver involves several layers of complexity, such as support for different potentials, single particle bases, sampling models, etc., which may easily lead to a combinatoric disaster if the planning is not done right. 

This chapter will cover the underlaying assumptions and goals in Section \ref{sec:AssGoal}, the ..

\section{Underlying Assumptions and Goals}
\label{sec:AssGoal}

Thousands of lines of code should be written once and for all\footnote{This, however, is rarely the case. The code used in this thesis has been completely restructured four times.}. This section will cover the assumptions made and the goals set, preliminary to the coding process.

\subsection{Assumptions}
\label{sec:ass}

The code scheme was laid down based on the following assumptions

\cfbox{3cm-3pt}{
\begin{enumerate}[label=\textbf{(\roman{*})}, ref=(\roman{*}), align=left]
 \item The constituents of the simulated systems are either pure fermionic or pure bosinic. \label{it::ass::pureFermBos}
 \item A fermionic system is a two-level system described by one Slater determinant pr. spin eigenvalue\footnote{The actual code is developed to easily add support for $n$-level systems.}. \label{it::ass::2level}
 \item The trial wave function of a fermionic system is a single determinant. \label{it::ass::fermiSingleDet}
 \item A bosonic system is modeled by all particles being in the same assumed single particle ground state. \label{it::ass::bosCondensate}
 \vspace{0.3cm}
\end{enumerate}
}

Other assumptions, such as the time-independence of the Hamiltonian, arise due to the QMC solver, not the specific implementation, and are hence covered in Chapter \ref{ch:QMC}.

\subsection{Generalization Goals}
\label{sec:genGoals}

The implementation should be general for:

\cfbox{3.17cm-1pt}{
\begin{enumerate}[label=\textbf{(\roman{*})}, ref=(\roman{*}), align=left]
 \item Fermions and Bosons. \label{it::gen::FermiAndBoson}
 \item Anisotropic- and isotropic diffusion, i.e. Brute Force - or Importance sampling. \label{it::gen::BF_IS}
 \item Different minimization algorithms. 
 \item Any Jastrow factor.
 \item Any error estimation algorithm.
 \item Any single particle basis, including expanded single particle bases. \label{it::gen::SP_basis}
 \item Any combination of any potentials. \label{it::gen::pot}
 \vspace{0.2cm}
\end{enumerate}
}
 
In addition, the following constraint is set on solvers:

\cfbox{3.42cm-1pt}{
\begin{enumerate}[label=\textbf{(\roman{*})}, ref=(\roman{*}), align=left]
\setcounter{enumi}{7}
 \item Full numerical support for all values involving derivatives.\label{it::gen::numSupp}
 \vspace{0.2cm}
\end{enumerate}
}


The challenge is, despite the vast array of combinations, to preserve simplicity and structure as layers of complexity are added. If-testing inside the solvers in order to achieve generalization are in other words out of the question (see the next section).

\subsection{Optimization Goals}
\label{sec:optGoals}

Designed for the CPU, runtime optimizations are favored over memory optimizations. The following list may appear short, but every step brings immense amounts of complexity to the implementation

\cfbox{3.6cm}{
\begin{enumerate}[label=\textbf{(\roman{*})}, ref=(\roman{*}), align=left]
 \item Identical values should never be re-calculated. \label{it::opt::reCalc}
 \item Generalization should not be achieved through if-tests in repeated function calls, but rather through polymorphism (see Section \ref{sec:typeCastPoly}). \label{it::opt::noIF}
 \item Linear scaling of runtime and number of CPUs for large simulations. \label{it::opt::parScale}
 \vspace{0.2cm}
\end{enumerate}
}

\clearpage
\section{Specifics Regarding Generalization}

Generalization is achieved through the use of deep object orientation, i.e. polymorphism, rather than if-testing. These concepts are described in Section \ref{sec:OO}. The assumptions listed in Section \ref{sec:ass} are applied if not otherwise is stated.

For details regarding the implementation of methods, see the code in \cite{libBorealisCode}.

\subsection{Generalization Goals \ref{it::gen::FermiAndBoson}-\ref{it::gen::pot}}

As discussed in Section \ref{sec:manyBodyWFs}, the mathematical difference between fermions and bosons (of importance to QMC) is how the many-body wave functions are constructed from the single-particle basis. In the case of fermions, the expression is given in terms of two Slater determinants, while for bosons, it is simply the product of all states.

\vspace{0.2cm}
\begin{lstlisting}
double Fermions::get_spatial_wf(const Walker* walker) {
    using namespace arma;
    
    //Spin up times Spin down (determinants)
    return det(walker->phi(span(0, n2 - 1), span())) * det(walker->phi(span(n2, n_p - 1), span()));
}
\end{lstlisting}

\begin{lstlisting}
double Bosons::get_spatial_wf(const Walker* walker) {
 
    double wf = 1;
 
    //Using the phi matrix as a vector in the case of bosons.
    //Assuming all particles to occupy the same single particle state (neglecting permutations).
    for (int i = 0; i < n_p; i++){
      wf *= walker->phi(i);
    }
    
    return wf;
}
\end{lstlisting}

Fermion/Boson overloaded pure virtual methods exist for all methods involving evaluation of the many body wave function, e.g. the spatial ratio and the Laplacian sum. When the \verb+QMC+ solver asks the \verb+System*+ object for a spatial ratio, depending on whether Fermions or Bosons are loaded run-time, the Fermion or Boson spatial ratio is evaluated. 

This way of splitting the system class also takes care of optimization goal \ref{it::opt::noIF} in Section \ref{sec:optGoals} regarding no use of repeating if-tests. 

Similar polymorphic splitting is introduced in the following classes:

\begin{listliketab}
\storestyleof{itemize}
 \begin{tabular}{l l}
 \textbullet \,\verb+Orbitals+       & Hydrogen orbitals, harmonic oscillator, etc. \\
 \textbullet \,\verb+BasisFunctions+ & Stand-alone single particle wave functions. Initialized by \verb+Orbitals+. \\
 \textbullet \,\verb+Sampling+       & Brute force - or importance sampling. \\
 \textbullet \,\verb+Diffusion+      & Isotropic or Fokker-Planck. Automatically selected by \verb+Sampling+. \\
 \textbullet \,\verb+ErrorEstimator+ & Simple or Blocking. \\
 \textbullet \,\verb+Jastrow+        & Pad√© Jastrow - or no Jastrow factor. \\
 \textbullet \,\verb+QMC+            & VMC or DMC. \\
 \textbullet \,\verb+Minimizer+      & ASGD. Support for adding additional minimizers. \\
 \end{tabular}
\end{listliketab}

Implementing a new Jastrow Factor done simply by creating a new subclass of \verb+Jastrow+: Simple yet versatile. For more details, see Section \ref{sec:OO}. The splitting done in \verb+QMC+ is done to avoid rewriting a lot of general QMC code.

A detailed description of the generalization of potentials (point \ref{it::gen::pot}) is given in Section \ref{sec:typeCastPoly}.

\subsection{Generalization Goal \ref{it::gen::SP_basis} and Expanded bases}

An expanded single particle basis is implemented as a subclass of the \verb+Orbitals+ superclass. It is designed as a wrapper to an \verb+Orbitals+ subclass containing basis elements $\phi_\alpha(r_j)$. In addition to these elements, the class has a set of expansion coefficients $C_{\gamma\alpha}$ in which the new basis elements are constructed

\begin{equation}
\label{eq:ExpBasisSP}
 \psi_\gamma^\mathrm{Exp.}(r_j) = \sum_{\alpha=0}^{B - 1} C_{\gamma\alpha}\phi_\alpha(r_j) 
\end{equation}

where $B$ is the size of the expanded basis.

\vspace{0.5cm}
\begin{lstlisting}[language=C++]
class ExpandedBasis : public Orbitals {

...

protected:

    int basis_size;
    arma::mat coeffs;
    Orbitals* basis;
    
    //Hartree-Fock
    void calculate_coefficients();

};

\end{lstlisting}

In order to calculate the expansion coefficients, a \textit{Hartree-Fock} solver has been implemented. For a given set of orbitals, e.g. harmonic oscillator, everything that is needed in order to obtain an expanded basis, is to implement expressions for the one-body - and two-body interaction elements.

The implementation of Eq.~(\ref{eq:ExpBasisSP}) are done by overloading the original \verb+Orbitals.phi+ method

\vspace{0.5cm}
\begin{lstlisting}
double ExpandedBasis::phi(const Walker* walker, int particle, int q_num) {

    double value = 0;
    
    //Dividing basis_size by half assuming a two-level system.
    //In case of Bosons, expanding s.p. w.f. does not make sence.
    for (int m = 0; m < basis_size/2; m++) {
        value += coeffs(q_num, m) * basis->phi(walker, particle, m);
    }

    return value;

}
\end{lstlisting}

See Section \textbf{REF HF} for details regarding Hartree-Fock theory.

\subsection{Generalization Goal \ref{it::gen::numSupp}}

Functionality for evaluating derivatives numerically is important for two reasons; the first being debugging, the second being the cases where no closed-form expressions for the derivatives can be obtained\footnote{In most cases they can, however, in some cases the expressions are so heavy that they are not practical to use.}.

As an example, \verb+Orbitals.dell_phi+, which returns a single particle derivative, is virtual, and can be overloaded to call the numerical derivative implementation \verb+Orbitals.num_diff+\footnote{An alternative would be to perform the derivative on the full many-body wave function, however, this would demand another layer of polymorphism and make the code less intuitive.}. The same goes for the Jastrow factor and the variational derivatives in the minimizers. Similar implementations exist for the Laplacian.

The numerical implementations are finite difference schemes with error proportional to the squared step length.

\section{Optimizations due to a Single two-level Determinant}

Assumption \ref{it::ass::fermiSingleDet} unlocks the possibility to optimize the expressions involving the Slater-determinant dramatically. Similar optimizations for bosons due to assumption \ref{it::ass::bosCondensate} are considered trivial and will not be covered in detail. See the code in \cite{libBorealisCode} for details regarding bosons.

The full trial wave function $\Psi_T$ is given by Eq.~(\ref{eq:singleDeterminantTWF}). I will drop the function arguments to clean up the expressions. Written in terms of a spatial function $|D|$ and a Jastrow function $J$, we get

\newcommand{\PT}{|D^\uparrow||D^\downarrow|J}
\newcommand{\Du}{|D^\uparrow|}
\newcommand{\Dd}{|D^\downarrow|}
\newcommand{\Da}{|D^\alpha|}

\begin{equation}
 \Psi_T = \PT
\end{equation}

The Quantum Force becomes

\begin{eqnarray}
 F_i &=& 2\frac{\nabla_i\left(\PT\right)}{\PT} \nonumber\\
     &=& 2\left(\frac{\nabla_i\Du}{\Du} + \frac{\nabla_i\Dd}{\Dd} + \frac{\nabla_i J}{J}\right) \nonumber \\
     &=& 2\left(\frac{\nabla_i\Da}{\Da} + \frac{\nabla_i J}{J}\right)
\end{eqnarray}

where $\alpha$ is the spin configuration of particle $i$. The counterpart to $\alpha$ has no dependence of particle $i$ and will hence be zero in the expression above.

Equally for the Laplacians used in the local energy, we get

\begin{eqnarray}
 E_L &=& \sum_i \frac{1}{\Psi_T}\nabla^2_i \Psi_T + V \label{eq:localEsum}\\
  \frac{1}{\Psi_T}\nabla^2_i\Psi_T &=&  \frac{1}{\PT}\nabla^2_i \PT \nonumber\\
  &=& \frac{\nabla^2_i \Du}{\Du} + \frac{\nabla^2_i \Dd}{\Dd} + \frac{\nabla^2_i J}{J} \nonumber\\
  && +\,\, 2\frac{\left(\nabla_i\Du\right)\left(\nabla_i\Dd\right)}{\Du\Dd} + 2 \frac{\left(\nabla_i\Du\right)\left(\nabla_i J\right)}{\Du J} + 2 \frac{\left(\nabla_i\Dd\right)\left(\nabla_i J\right)}{\Dd J} \nonumber\\
  &=& \frac{\nabla^2_i \Da}{\Da} + \frac{\nabla^2_i J}{J} + 2\frac{\nabla_i\Da}{\Da} \frac{\nabla_i J}{J}
\end{eqnarray}

Finally, the expression for the $R_\psi$ ratio for Metropolis is

\begin{eqnarray}
 R_\psi &=& \frac{\Psi_T^\mathrm{new}}{\Psi_T^\mathrm{old}} \nonumber\\
        &=& \frac{\Du^\mathrm{new} \Dd^\mathrm{new} J^\mathrm{new}}{\Du^\mathrm{old} \Dd^\mathrm{old} J^\mathrm{old}} \nonumber\\
        &=& \frac{\Da^\mathrm{new}}{\Da^\mathrm{old}}\frac{J^\mathrm{new}}{J^\mathrm{old}} \label{eq:RpsiOpt}
\end{eqnarray}

where either the spin up or the spin down determinant is unchanged by the step. 

From these expressions it is clear that we get a halving of the dimensionality of the system by splitting the Slater determinants. Calculation the determinant of a $N\times N$ matrix is $\mathcal{O}(N^2)$ floating point operations (flops). This implies a speedup of four in estimating the determinants alone. 

\section{Optimizations due to Single-particle Moves}

Moving only one particle at the time (see the diffusion algorithm in Fig. \ref{fig:diffFlowChart}), means changing a single row in the Slater-determinant at the time. Changes to a single row means that almost every \textit{co-factor} remains unchanged. Since all of the expressions deduced in the previous section contains ratios of the spatial wave functions, expressing these determinants in terms of their co-factors should reveal cancellation of terms. 

\subsection{Optimizing the Slater ratios}
\label{sec:optSlaterRat}

The inverse of the Slater-matrix is given in terms of its \textit{adjugate} by the following relation \cite{linAlg} (spin-configuration parameter $\alpha$ will be skipped for now)

\begin{equation*}
 D^{-1} = \frac{1}{|D|}\mathrm{adj} D
\end{equation*}

The adjugate of a matrix is the transpose of the cofactor matrix $C$. Given in terms of matrix element equations, we have

\begin{eqnarray}
 D^{-1}_{ij} &=& \frac{C_{ji}}{|D|}\label{eq:invExpCofac} \\
 D_{ji} &=& \phi_i(r_j) \label{eq:slaterMatPhi}
\end{eqnarray}

Moreover, the determinant can be expressed as a \textit{cofactor expansion} around row $j$ (Kramer's rule)

\begin{equation}
\label{eq:cofacExp}
 |D| = \sum_i D_{ji} C_{ji}.
\end{equation}

The spatial part of the $R_\psi$ ratio is then (inserting Eq.~(\ref{eq:cofacExp}) into Eq.~(\ref{eq:RpsiOpt}))

\begin{equation}
\label{eq:RpsiCofac}
 R_S = \frac{\sum_i D_{ji}^\mathrm{new}C_{ji}^\mathrm{new}}{\sum_i D_{ji}^\mathrm{old}C_{ji}^\mathrm{old}}
\end{equation}

Let $j$ represent the moved particle, the $j$'th column of the cofactor matrix is unchanged when the particle moves (column $j$ depends on every column but its own). In other words,

\begin{equation}
 C_{ji}^\mathrm{new} = C_{ji}^\mathrm{old} = (D^\mathrm{old}_{ij})^{-1}|D^\mathrm{old}|,
\end{equation}

where the inverse relation of Eq.~(\ref{eq:invExpCofac}) has been used. Inserting this into Eq.~(\ref{eq:RpsiCofac}) yields

\begin{eqnarray}
  R_S &=& \frac{|D^\mathrm{old}|}{|D^\mathrm{old}|}\frac{\sum_i D_{ji}^\mathrm{new}(D_{ij}^\mathrm{old})^{-1}}{\sum_i D_{ji}^\mathrm{old}(D_{ij}^\mathrm{old})^{-1}} \nonumber\\
         &=& \frac{\sum_i D_{ji}^\mathrm{new}(D_{ij}^\mathrm{old})^{-1}}{I_{jj}} \nonumber
\end{eqnarray}

The diagonal element of the identity matrix is by definition unity. Inserting this fact combined with the relation from Eq.~(\ref{eq:slaterMatPhi}) yields the optimized expression for the ratio

\cfbox{4cm-4pt}{
\begin{equation}
\label{eq:spatialRatioDet}
 R_S = \sum_i \phi_i(r_j^\mathrm{new})(D_{ij}^\mathrm{old})^{-1}
\end{equation}
}

where $j$ is the currently moved particle. The sum $i$ spans the Slater matrix whose spin value matches that of particle $j$.


Similar reductions can be applied to all the Slater ratio expressions from the previous section, see refs. \cite{abInitioMC, morten}:

\cfbox{4cm}{
\begin{eqnarray}
 \frac{\nabla_i|D|}{|D|} &=& \sum_{k} \nabla_i\phi_k(r_i^\mathrm{new})(D_{ki}^\mathrm{new})^{-1} \\
  \frac{\nabla_i^2|D|}{|D|} &=& \sum_{k} \nabla_i^2\phi_k(r_i^\mathrm{new})(D_{ki}^\mathrm{new})^{-1}
\end{eqnarray}
}

where again, $k$ spans the Slater matrix whose spin values match that of the moved particle. Unlike for the ratio, $N/2$ of the gradients needs to be recalculated once a particle is moved (with $N$ being the number of particles). This is due to the fact that it is the new Slater inverse that is used, and not the old.

Closed form expressions for the derivatives and Laplacians of the single particle may be implemented and accessed when calling these functions to avoid expensive numerical calculations. See Appendices \textbf{REF SYMPYGEN} for a tabulation of closed form expressions.

\subsection{Optimizing the Inverse}
\label{sec:optInv}

One might question the efficiency of calculating inverse matrices compared to brute force estimation of the determinants. However, as for the ratio in Eq.~(\ref{eq:spatialRatioDet}), using co-factor expansions we can construct an updating algorithm which dramatically decreases the cost of calculating the inverse of the new Slater matrix.

Let $i$ be the currently moved particle, the new inverse is given in terms of the old by the following expression \cite{abInitioMC, morten}

\cfbox{4.5cm-5pt}{
\begin{eqnarray}
 \tilde I_{ij} &=& \sum_l D_{il}^\mathrm{new}(D_{lj}^\mathrm{old})^{-1} \\
 \ \label{eq:Itilde}\
 (D_{kj}^\mathrm{new})^{-1} &=& (D_{kj}^\mathrm{old})^{-1} - \frac{1}{R_S}(D_{ji}^\mathrm{old})^{-1}\tilde I_{ij} \qquad\qquad j \ne i\\
 (D_{ki}^\mathrm{new})^{-1} &=& \frac{1}{R_S}(D_{ki}^\mathrm{old})^{-1} \qquad\qquad\qquad\qquad\qquad\,\, \mathrm{else} \\
 \ \nonumber
\end{eqnarray}
}

This reduces the cost of calculating the inverse by an order of magnitude down to $\mathcal{O}(N^2)$.

Further optimization can be achieved by calculating the $\tilde I$ vector for particle $i$ prior to performing the loop over $k$ and $j$. Again, this loop should only update the inverse Slater matrix whose spin value correspond to that of the moved particle.

\subsection{Optimizing the Pad√© Jastrow factor Ratio}

As for the Green's function ratio is Eq.~(\ref{eq:MetropolisHastings}), the ratio between two Jastrow factors are best calculating as exponentiation the logarithm

\begin{eqnarray}
 \log \frac{J^\mathrm{new}}{J^\mathrm{old}} &=& \sum_{k<j = 1}^N \frac{a_{kj}r^\mathrm{new}_{kj}}{1 + \beta r^\mathrm{new}_{kj}} - \frac{a_{kj}r^\mathrm{old}_{kj}}{1 + \beta r^\mathrm{old}_{kj}} \\
                      &\equiv& \sum_{k<j = 1}^N g^\mathrm{new}_{kj} - g^\mathrm{old}_{kj} \label{eq:jastrowRatSTD}
\end{eqnarray}

The relative distances $r_{kj}$ behave much like the cofactors in Section \ref{sec:optSlaterRat}; changing $r_i$ only changes $r_{ij}$, in other words

\begin{equation}
 r^\mathrm{new}_{kj} = r^\mathrm{old}_{kj} \qquad k \ne i 
\end{equation}

which inserted into Eq.~(\ref{eq:jastrowRatSTD}) yields

\begin{eqnarray}
  \log\frac{J^\mathrm{new}}{J^\mathrm{old}} &=& \sum_{k<j \ne i} g^\mathrm{old}_{kj} - g^\mathrm{old}_{kj} + \sum_{j = 1}^N g^\mathrm{new}_{ij} - g^\mathrm{old}_{ij} \nonumber\\
                                            &=& \sum_{j = 1}^N a_{ij}\left(\frac{r^\mathrm{new}_{ij}}{1 + \beta r^\mathrm{new}_{ij}} - \frac{r^\mathrm{old}_{ij}}{1 + \beta r^\mathrm{old}_{ij}}\right)
\end{eqnarray}

Exponentiating both sides reveals the final optimized ratio 

\cfbox{4.6cm-2pt}{
\begin{equation}
 \frac{J^\mathrm{new}}{J^\mathrm{old}} = \exp \left[\sum_{j = 1}^N a_{ij}\left(\frac{r^\mathrm{new}_{ij}}{1 + \beta r^\mathrm{new}_{ij}} - \frac{r^\mathrm{old}_{ij}}{1 + \beta r^\mathrm{old}_{ij}}\right)\right]
\end{equation}
}


\section{Optimizing the Pad√© Jastrow Derivative Ratios}

The shape of the Pad√© Jastrow factor is general in the sense that its shape is independent of the system at hand. Calculating closed form expressions for the derivatives is then a process which can be done once and for all. 

\subsection{The Gradient}

Using the notation of Eq.~(\ref{eq:jastrowRatSTD}), the $x$ component of the Pad√© Jastrow gradient ratio for particle $i$ then becomes

\begin{equation}
 \frac{1}{J}\frac{\partial J}{\partial x_i} = \frac{1}{\prod_{k < l}\exp g_{kl}}\frac{\partial }{\partial x_i}\prod_{k < l}\exp g_{kl} 
\end{equation}

Using the product rule, only terms with $k$ or $l$ equal to $i$ survive the differentiation. In addition, the terms independent of $i$ will cancel the corresponding terms in the denominator. In other words, 

\begin{eqnarray}
  \frac{1}{J}\frac{\partial J}{\partial x_i} &=& \sum_{k \ne i} \frac{1}{\exp g_{ik}}\frac{\partial}{\partial x_i} \exp g_{ik} \nonumber\\
  &=& \sum_{k \ne i} \frac{1}{\exp g_{ik}}\exp g_{ik}\frac{\partial g_{ik}}{\partial x_i} \nonumber \\
  &=& \sum_{k \ne i} \frac{\partial g_{ik}}{\partial x_i} \nonumber \\
  &=& \sum_{k \ne i} \frac{\partial g_{ik}}{\partial r_{ik}}\frac{\partial r_{ik}}{\partial x_i}
\end{eqnarray}

\begin{eqnarray}
 \frac{\partial g_{ik}}{\partial r_{ik}} &=& \frac{\partial }{\partial r_{ik}} \left(\frac{a_{ik}r_{ik}}{1 + \beta r_{ik}}\right) \nonumber\\
  &=& \frac{a_{ik}}{1 + \beta r_{ik}} - \frac{a_{ik}r_{ik}}{(1 + \beta r_{ik})^2}\beta \nonumber \\
  &=& \frac{a_{ik}(1 + \beta r_{ik}) - a_{ik}\beta r_{ik}}{(1 + \beta r_{ik})^2}  \nonumber \\
  &=& \frac{a_{ik}}{(1 + \beta r_{ik})^2} \label{eq:jastrowDgikDrik}
\end{eqnarray}

\begin{eqnarray}
 \frac{\partial r_{ik}}{\partial x_i} &=& \frac{\partial }{\partial x_i} \sqrt{(x_i - x_k)^2 + (y_i - y_k)^2 + (z_i - z_k)^2} \nonumber \\
  &=& \frac{1}{2} 2(x_i - x_k) / \sqrt{(x_i - x_k)^2 + (y_i - y_k)^2 + (z_i - z_k)^2} \nonumber \\
  &=& \frac{x_i - x_k}{r_{ik}}
\end{eqnarray}

Combining these expressions yields

\begin{equation}
\label{eq:jastrowDerivX}
 \frac{1}{J}\frac{\partial J}{\partial x_i} = \sum_{k \ne i} \frac{a_{ik}}{r_{ik}}\frac{x_i - x_k}{(1 + \beta r_{ik})^2}
\end{equation}

When changing Cartesian variable in the differentiation, the only change to the expression is that the corresponding Cartesian variable changes in the numerator of Eq.~(\ref{eq:jastrowDerivX}). In other words, generalizing to the full gradient is done by substituting the Cartesian difference with the position vector difference.

\cfbox{4.8cm}{
\begin{equation}
\label{eq:jastrowGradFull}
 \frac{\nabla_i J}{J} = \sum_{k \ne i = 1}^N \frac{a_{ik}}{r_{ik}}\frac{\vec r_i - \vec r_k}{(1 + \beta r_{ik})^2}
\end{equation}
}


\subsection{The Laplacian}

The method of deducing the closed form expression for the Laplacian of the Pad√© Jastrow factor is identical to that of the gradient. The full calculation is done in ref. \cite{morten}. The expression becomes

\begin{equation}
\label{eq:jastrowLaplRaw}
 \frac{\nabla^2_i J}{J} = \left| \frac{\nabla_i J}{J}\right|^2 + \sum_{k \ne i = 1}^N \left(\frac{d-1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}} + \frac{\partial^2 g_{ik}}{\partial r_{ik}^2}\right) 
\end{equation}

where $d$ is the number of dimensions, arising due to the fact that the Laplacian, unlike the gradient, is a summation of contributions from all dimensions. A simple differentiation of Eq.~(\ref{eq:jastrowDgikDrik}) with respect to $r_{ik}$ yields

\begin{equation}
\label{eq:jastrowD2gikDrik2}
 \frac{\partial^2 g_{ik}}{\partial r_{ik}^2} = -\frac{2a_{ik}\beta}{(1 + \beta r_{ik})^3}
\end{equation}

Inserting Eq.~(\ref{eq:jastrowDgikDrik}) and Eq.~(\ref{eq:jastrowD2gikDrik2}) into Eq.~(\ref{eq:jastrowLaplRaw}) yields

\begin{eqnarray}
 \frac{\nabla^2_i J}{J} &=& \left| \frac{\nabla_i J}{J}\right|^2 + \sum_{k \ne i = 1}^N \left(\frac{d-1}{r_{ik}}\frac{a_{ik}}{(1 + \beta r_{ik})^2} - \frac{2a_{ik}\beta}{(1 + \beta r_{ik})^3}\right) \nonumber\\
  &=& \left| \frac{\nabla_i J}{J}\right|^2 + \sum_{k \ne i = 1}^N a_{ik}\frac{(d-1)(1 + \beta r_{ik}) - 2\beta r_{ik}}{r_{ik}(1 + \beta r_{ik})^3} \nonumber
\end{eqnarray}

which with a little cleanup results in

\begin{equation}
 \frac{\nabla^2_i J}{J} = \left| \frac{\nabla_i J}{J}\right|^2 + \sum_{k \ne i} a_{ik}\frac{(d-1) + (d-3)\beta r_{ik}}{r_{ik}(1 + \beta r_{ik})^3}
\end{equation}

The local energy calculation needs the sum of the Laplacians for all particles (see Eq.~(\ref{eq:localEsum})). In other words, the quantity of interest becomes

\begin{equation}
 \sum_i \frac{\nabla^2_i J}{J} = \sum_i \left|\frac{\nabla_i J}{J}\right|^2 + \sum_{i \ne k}^N a_{ik}\frac{(d-1) + (d-3)\beta r_{ik}}{r_{ik}(1 + \beta r_{ik})^3}
\end{equation}

However, due to the symmetry of $r_{ik}$, the second term count equal values twice. We can therefore optimize the calculation by only summing for $k>i$, and rather multiply the sum by two. Bringing it all together, we get

\cfbox{5cm}{
\begin{equation}
\label{eq:jastrowLaplFin}
 \sum_i \frac{\nabla^2_i J}{J} = \sum_i \left|\frac{\nabla_i J}{J}\right|^2 + 2\sum_{i < k}^N a_{ik}\frac{(d-1) + (d-3)\beta r_{ik}}{r_{ik}(1 + \beta r_{ik})^3}
\end{equation}
}


\section{Tabulating Recalculated Data}

The optimizations covered in this section will exclusively arise from point \ref{it::opt::reCalc} in section \ref{sec:optGoals}. Avoiding recalculating expressions also include exploiting, such as was done in the Pad√© Jastrow Laplacian in Eq.~(\ref{eq:jastrowLaplFin}).

rrel, r2, phimatrix, delphi, oppdatere jastrow gradient, qnum indie terms ++ a s a s d a d sd f s f s a sa dsa sda a a  ads ad  gf asrfs asdf as fas  a asd asd as 



\subsection{The relative distance matrix}

In the discussions regarding the optimization of the Jastrow ratio, it became clear that moving one particle only changed $N$ of the relative distances. Storing these values in a matrix, consequently updating the values corresponding to the moved particle, ensures that these values are calculated once and for all.

\begin{equation}
r_{rel} = r_{rel}^T = \left( \begin{array}{ccccc}
0 & r_{12} & r_{13} & \cdots & r_{1N} \\
 & 0 & r_{23} & \cdots & r_{2N}  \\
 &  & \ddots & \ddots & \vdots \\
 & \cdots &  & 0 & r_{(N-1)N} \\
 &  &  &  & 0\end{array} \right).
\end{equation}

\begin{lstlisting}
 void Sampling::update_pos(const Walker* walker_pre, Walker* walker_post, int particle) const {

    ...

    //Updating the part of the r_rel matrix which is changed by moving the [particle]
    for (int j = 0; j < n_p; j++) {
        if (j != particle) {
            walker_post->r_rel(particle, j) = walker_post->r_rel(j, particle)
                    = walker_post->calc_r_rel(particle, j);
        }
    }
    
    ...

}
\end{lstlisting}


Functions such as \verb+Coulomb.get_potential_energy+ and all of the evaluation functions of the Jastrow Factor can then simply access these matrix elements.

Similar storage has been done for the squared distance vector. 

\subsection{The Slater related matrices}
\label{sec:storeSlater}

Apart from the inverse, whose optimization was covered in Section \ref{sec:optInv}, calculating the single particle wave functions and its gradients are the most expensive operations of the QMC algorithm.

Storing these function values in a matrices representing the Slater Matrix and its derivatives, will ensure that these values never gets recalculated. 

\begin{equation}
\label{eq:slaterConcat}
 D  \equiv \left[D^\uparrow\,D^\downarrow\right] = \left[ \begin{array}{cccc}
\phi_1(r_0)     & \phi_1(r_1)     & \cdots & \phi_1(r_N)       \\
\phi_2(r_0)     & \phi_2(r_1)     & \cdots & \phi_2(r_N)       \\
\vdots          & \vdots          &        & \vdots            \\
\phi_{N/2}(r_0) & \phi_{N/2}(r_1) & \cdots & \phi_{N/2}(r_N)   \end{array} \right]
\end{equation}

\begin{equation}
\label{eq:slaterDellConcat}
 \nabla D \equiv \left[ \begin{array}{cccc}
\nabla \phi_1(r_0)     & \nabla \phi_1(r_1)     & \cdots & \nabla \phi_1(r_N)       \\
\nabla \phi_2(r_0)     & \nabla \phi_2(r_1)     & \cdots & \nabla \phi_2(r_N)       \\
\vdots                 & \vdots                 &        & \vdots                   \\
\nabla \phi_{N/2}(r_0) & \nabla \phi_{N/2}(r_1) & \cdots & \nabla \phi_{N/2}(r_N)   \end{array} \right]
\end{equation}

\subsection{Avoiding spin tests}

Since the slater determinant is split by spin eigenvalues, the same splitting occurs in the inverse, the Slater matrix etc. The brute force implementation is to if-test which of the matrices to access. In the case of a two-level system we get

\begin{equation}
\begin{array}{cl}
 i < N/2 & D^\uparrow(i) \\
 i \ge N/2 & D^\downarrow(i - N/2)
\end{array} 
\end{equation}

However, simply concatenating the Slater related matrices solves the entire problem. This has already been done in Eq.~(\ref{eq:slaterConcat}) and Eq.~(\ref{eq:slaterDellConcat}). Applying this to the inverse yields

\begin{equation}
 D^{-1} \equiv \left[(D^\uparrow)^{-1}\,(D^\downarrow)^{-1}\right]
\end{equation}

Leaving no if-tests required in order to sort the spin splitting. In e.g. the updating algorithm for the inverse, we must now simply keep track of which part we need to update by selecting a start and stop criteria, the rest of the code is general.

\subsection{The Pad√© Jastrow gradient}
\label{sec:optJastGrad}

Consider Eq.~(\ref{eq:jastrowGradFull}). Defining

\begin{equation}
 \mathrm{d}\vec J_{ik} = -\mathrm{d}\vec J_{ki} \equiv \frac{a_{ik}}{r_{ik}}\frac{\vec r_i - \vec r_k}{(1 + \beta r_{ik})^2},
\end{equation}

the gradient can be written in a more compact form

\begin{equation}
 \frac{\nabla_i J}{J} = \sum_{k \ne i = 1}^N \mathrm{d}\vec J_{ik}.
\end{equation}

As for the relative distances, storing the elements and exploiting the symmetry properties, only half the total elements needs to be calculated. 

\begin{equation}
\label{eq:jastrowDJ}
 \mathrm{d}\vec J = -\mathrm{d}\vec J^T \equiv \left( \begin{array}{ccccc}
0 & \mathrm{d}\vec J_{12} & \mathrm{d}\vec J_{13} & \cdots & \mathrm{d}\vec J_{1N} \\
 & 0 & \mathrm{d}\vec J_{23} & \cdots & \mathrm{d}\vec J_{2N}  \\
 &  & \ddots & \ddots & \vdots \\
 & (-) &  & 0 & \mathrm{d}\vec J_{(N-1)N} \\
 &  &  &  & 0\end{array} \right).
\end{equation}

\begin{lstlisting}
void Pade_Jastrow::get_dJ_matrix(Walker* walker, int i) const {
    
    //for all j != i
    b_ij = 1.0 + beta * walker->r_rel(i, j);
    factor = a(i, j) / (walker->r_rel(i, j) * b_ij * b_ij);
    for (int k = 0; k < dim; k++) {
        walker->dJ(i, j, k) = (walker->r(i, k) - walker->r(j, k)) * factor;
        walker->dJ(j, i, k) = -walker->dJ(i, j, k);
    }
}
\end{lstlisting}


Calculating the gradient is now only a matter of summing the rows of the matrix in Eq.~(\ref{eq:jastrowDJ}). On a more careful notice, we can further optimize the calculation of the gradient by taking into account that we have access to the previous gradient

\begin{eqnarray}
  \frac{\nabla_i J^\mathrm{old}}{J^\mathrm{old}} &=& \sum_{k \ne i = 1}^N \mathrm{d}\vec J^\mathrm{old}_{ik} \\
  \frac{\nabla_i J^\mathrm{new}}{J^\mathrm{new}} &=& \sum_{k \ne i = 1}^N \mathrm{d}\vec J^\mathrm{new}_{ik} \label{eq:jastrowGradDJnew}
\end{eqnarray}

By moving particle $p$ in QMC, only a single row and column of the $\mathrm{d}vec J$ matrix changes. Assuming that $i\ne p$, only a single term from the new matrix is required

\begin{eqnarray}
 \frac{\nabla_{i\ne p} J^\mathrm{new}}{J^\mathrm{new}} &=& \sum_{k \ne i \ne p} \mathrm{d}\vec J^\mathrm{old}_{ik} + \mathrm{d}\vec J^\mathrm{new}_{ip} \\
 &=& \left[\sum_{k \ne i \ne p} \mathrm{d}\vec J^\mathrm{old}_{ik} + \mathrm{d}\vec J^\mathrm{old}_{ip}\right] - \mathrm{d}\vec J^\mathrm{old}_{ip} + \mathrm{d}\vec J^\mathrm{new}_{ip} \\
 &=& \frac{\nabla_i J^\mathrm{old}}{J^\mathrm{old}} - \mathrm{d}\vec J^\mathrm{old}_{ip} + \mathrm{d}\vec J^\mathrm{new}_{ip}
\end{eqnarray}

reducing the calculation to three flops. For the case $i=p$, the entire sum as in Eq.~(\ref{eq:jastrowGradDJnew}) must be calculated. 

\begin{lstlisting}
void Pade_Jastrow::get_grad(const Walker* walker_pre, Walker* walker_post, int p) const {
    double sum;

    //i < p
    for (int i = 0; i < p; i++) {
        for (int k = 0; k < dim; k++) {
            walker_post->jast_grad(i, k) = walker_pre->jast_grad(i, k)
                    + walker_post->dJ(i, p, k) - walker_pre->dJ(i, p, k);
        }
    }

    //i = p
    for (int k = 0; k < dim; k++) {

        sum = 0;
        for (int j = 0; j < p; j++) {
            sum += walker_post->dJ(p, j, k);
        }

        for (int j = p + 1; j < n_p; j++) {
            sum += walker_post->dJ(p, j, k);
        }

        walker_post->jast_grad(p, k) = sum;

    }

    //i > p
    for (int i = p + 1; i < n_p; i++) {
        for (int k = 0; k < dim; k++) {
            walker_post->jast_grad(i, k) = walker_pre->jast_grad(i, k) 
                    + walker_post->dJ(i, p, k) - walker_pre->dJ(i, p, k);
        }
    }

}
\end{lstlisting}

This optimization scales very well with increasing number of particles.


\subsection{The single-particle Wave Functions}
\label{sec:optSPWFqnumIndie}

A single particle wave function is defined by a position in space $r$ and a quantum number $q$. A QMC walker holds the position of all particles, so in the case of QMC, the parameters defining the single particle wave function is a walker, a particle number $i$ and of course the quantum number (for the gradient we also need the dimension).

For systems of many particles, the function call \verb+Orbitals.phi(walker, i, qnum)+ needs to figure out which expression is related to which quantum number. The brute force implementation is to simply if-test on the quantum number, and calculate the corresponding expression inside the correct if-test.

\begin{lstlisting}
double AlphaHarmonicOscillator::phi(const Walker* walker, int particle, int q_num) {
    
    //Ground state of the harmonic oscillator
    if (q_num == 0){
      return exp(-0.5*w*walker->get_r_i2(i));
    }
    
    ...
    
}
\end{lstlisting}

This is however inefficient when the number of particles numbers become large. A more efficient implementation is to implement the single particle wave function expressions as \verb+BasisFunctions+ subclasses. \verb+BasisFunctions+ has one pure virtual function, \textit{eval}, which then only needs the particle number $i$ and the walker. The class itself is defined by the quantum number. 

The following is an example of the harmonic oscillator single particle wave function for quantum number $q=1$.

\begin{lstlisting}
double HarmonicOscillator_1::eval(const Walker* walker, int i) {

    y = walker->r(i, 1);
    
    //y*exp(-k^2*r^2/2)
    
    H = y;
    return H*exp(-0.5*w*walker->get_r_i2(i));
    
}
\end{lstlisting}

These objects representing single particle wave functions can be loaded into an array, such that element $q$ corresponds to the \verb+BasisFunctions+ object representing this quantum number. The new \verb+Orbitals+ function then becomes

\begin{lstlisting}
double Orbitals::phi(const Walker* walker, int particle, int q_num) {
    return basis_functions[q_num]->eval(walker, particle);
}
\end{lstlisting}

with no if-tests required.

Other more specific optimizations can be implemented, so called \textit{quantum number independent factors}, $\overline{Q}_i$. When moving particle $p$, as discussed previously, a single column in the Slater matrix from Eq.~(\ref{eq:slaterConcat}) needs to be updated. All these terms are calculated in the same position. This implies that terms independent of the quantum number will be equal. Looking at the single particle states for harmonic oscillator and hydrogen listed in the Appendix, their exponential form results in that an exponential factor independent of the quantum number is present in all terms.

\begin{eqnarray}
\overline{Q}_i^\mathrm{H.O.} &=& e^{-\frac{1}{2}\alpha\omega r_i^2} \\
 \overline{Q}_i^\mathrm{Hyd.} &=& e^{-\frac{1}{n}\alpha Z r_i}
\end{eqnarray}

For hydrogen, we have a dependence on the principle quantum number $n$, however, for e.g $n=2$,  $20$ terms share this exponential factor. Calculating it once and for all saves $19$ exponential calls pr. particle pr. walker pr. cycle etc.

The implementation is rather simple. Upon moving a particle, the virtual function \\\verb+Orbitals.set_qnum_indie_terms+ is called, updating the value of e.g. an \verb+exp_factor+ pointer shared by all the loaded \verb+BasisFunctions+ objects and the \verb+Orbitals+ class.

\begin{lstlisting}
void AlphaHarmonicOscillator::set_qnum_indie_terms(const Walker * walker, int i) {
   
   //k2 = alpha*omega 
   *exp_factor = exp(-0.5 * (*k2) * walker->get_r_i2(i));
}

void hydrogenicOrbitals::set_qnum_indie_terms(const Walker* walker, int i) {

    //k = alpha*Z
    *exp_factor_n1 = exp(-(*k) * walker->get_r_i(i));
    *exp_factor_n2 = exp(-(*k) * walker->get_r_i(i) / 2);
    ...

}
\end{lstlisting}

The \verb+BasisFunctions+ objects can then simply access this value instead of calculating the exponential 

\begin{lstlisting}
double HarmonicOscillator_1::eval(const Walker* walker, int i) {

    y = walker->r(i, 1);
    
    //y*exp(-k^2*r^2/2)
    
    H = y;
    return H*(*exp_factor);
    
}

double lapl_hydrogenic_0::eval(const Walker* walker, int i) {
    
    //k*(k*r - 2)*exp(-k*r)/r
    
    psi = (*k)*((*k)*walker->get_r_i(i) - 2)/walker->get_r_i(i);
    return psi*(*exp_factor);
    
}
\end{lstlisting}

This optimization is an enormous speedup for many particle simulations.

All single particle expressions given are calculated using \textit{SymPy}. See Appendix \textbf{REF SYMPY} for details. For Quantum Dots, $84$ \verb+BasisFunctions+ objects are needed for a $42$-particle simulation, reducing the number of exponential calls with $83$ pr. particle pr. walker pr. cycle, which for an average DMC calculation results in $~24\cdot 10^9$ saved exponential calls.


\section{CPU Cache Optimization}

The \textit{CPU cache} is a limited amount of memory directly connected to the CPU designed to reduce the average time to access memory. Simply speaking, standard memory is slower than the CPU cache, as bits have to travel through the motherboard before it can be fed to the CPU (a so called \textit{bus}). 

Which values are held in the CPU cache is controlled by the compiler, however, if programmed poorly, the compiler will not be able to handle the cache storage optimally. Optimization tools exist in order to work around this, however, keeping the cache in mind from the beginning of the coding process can result in a much faster code. In the case of the QMC code, the most optimal use of the cache would be to have complete walkers ready in the cache at all times. 

The memory is sent to the cache as arrays, which means that storing walker data sequentially in memory is the way to go in order to make take full use of the processor cache. This is ensured by not using pointers within the walker objects, as pointers are free to be declared in any memory address.

\subsection{Disadvantage of Generalizing the code}

The size of the matrices within the walker objects are dynamically allocated in order to handle any number of particles and dimensions. At compile-time, there is no telling how much memory each walker will demand, and thus it is harder for the compiler to optimize the cache usage.

The alternative is to statically declare, i.e. declare with a fixed size known at compile time, the matrices to be of the maximum possible simulation size. This would however waste a ton of memory, and would render most applications impossible to run at a standard computer. A second alternative would be to re-compile the code every time the system variables are changed. This is not optimal in the case of a generalized solver.

\subsection{Consequences}

The QMC code has support for both kinds of scenarios. Compiling the source code with a main file which fixes the system variables will result in a more efficient executable. For the purpose of this thesis, this was not done; all system variables are controlled via a configuration script (see \verb+runQMC.py+ in ref. \cite{libBorealisCode}).







